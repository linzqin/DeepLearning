# 第一课：深度学习和神经网络

## 1 深度学习概论
### 1.1 什么是神经网络

![](./img/神经网络形象.jpg)        

1. ReLU：修正线性单元，修正是指取不小于0的值

### 1.2 用神经网络进行监督学习（supervised learning）
- 结构化数据：数据库。每个特征有清晰的定义
- 非结构化数据：如音频、图片、文本

### 1.3 深度学习为什么兴起
- 常规学习算法无法处理海量数据，会遇到「平台」
- 神经网络的规模和数据（带有标签的数据）的规模促进了深度学习

## 2 神经网络基础
### 2.1 二分类
- 二分类，最终结果得到一个「是or否」，实现一个分类器
- (x,y)：一个样本
    - x：nx维的特征向量
    - n/nx：一般用于标识输入的特征向量的维度
    - y：标签，0 or 1
    - m：训练集/测试集的样本数量
- 单个样本做**列向量**
    - X.shape=(nx,m)
    - Y.shape=(1,m)
### 2.2  logistic 回归
- 输出y，是一个概率，再0~1之间，所以会用到**sigmod函数**
    - y=sigmod(wx+b)

### 2.3  logistic 回归损失函数
- 损失：y'和y的差别

- **损失函数（Lose function）是衡量单个训练样本的表现，成本函数(Cost function)何亮全体样本的表现**
- logistic的损失函数形式如下：     

![](./img/损失函数.jpg)         

- 成本函数形式如下：     

![](./img/成本函数.jpg)         

### 2.4  梯度下降法
- 成本函数J是凹函数，这样就不会存在多个局部最优解      

![](./img/梯度下降.jpg)      

### 2.5 导数 && 2.6 更多导数的例子

### 2.7  计算图
- 如图所示，计算图(computation graph)是按照黑色笔记的步骤，从左到右的计算，而导数计算则是从右到左的顺序

2.8  计算图的导数计算

2.9  logistic 回归中的梯度下降法

2.10  m 个样本的梯度下降

2.11  向量化

2.12  向量化的更多例子

2.13  向量化 logistic 回归

2.14  向量化 logistic 回归的梯度输出

2.15  Python 中的广播

2.16  关于 python / numpy 向量的说明

2.17  Jupyter / Ipython 笔记本的快速指南

2.18  （选修）logistic 损失函数的解释






## 参考资料

- [课程的文字笔记撰写](https://github.com/fengdu78/deeplearning_ai_books)
- [课程目录](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)
